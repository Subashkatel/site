import Link from 'next/link'
import { TableOfContents } from '../../components/TableOfContents'

export const metadata = {
  title: 'Gradient Descent: From Theory to Modern Optimizers',
  description: 'A comprehensive mathematical and practical guide to gradient descent and its variants',
  date: '2025-01-11',
}

<div id="wrapper">
  <div id="left" className="container">
    <div className="flex flex-col">
      <p><Link href="/">‚Üê Back to home</Link></p>
      <div className="post-date">
        January 11, 2025
      </div>
      <TableOfContents />
    </div>
  </div>
  
  <div id="right" className="container">

# Gradient Descent: From Theory to Modern Optimizers

*A deep dive into the mathematical foundations and practical implementations of optimization algorithms that power modern machine learning.*

> **Note:** This is an AI-generated example post created to demonstrate blog formatting with LaTeX math, code blocks, and citations. It is not written by the site author.

---

## Introduction

<span style={{color: '#2563eb', fontWeight: '600'}}>Gradient descent is the workhorse of modern machine learning.</span> Whether you're training a simple linear regression or a massive transformer with billions of parameters, you're fundamentally solving an optimization problem using variants of gradient descent.

In this post, we'll explore:
- **Mathematical foundations** and convergence guarantees
- **Implementation details** with full Python code
- **Modern variants** (SGD, Momentum, Adam, AdaGrad)
- **Practical considerations** for real-world applications

---

## 1. Mathematical Foundation

### 1.1 The Optimization Problem

Given a differentiable function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, we want to find:

$$
x^* = \arg\min_{x \in \mathbb{R}^n} f(x)
$$

where $x^*$ is a <span style={{color: '#dc2626'}}>local (or global) minimum</span>.

### 1.2 The Gradient Descent Algorithm

The iterative update rule is:

$$
x_{t+1} = x_t - \alpha \nabla f(x_t)
$$

where:
- $x_t \in \mathbb{R}^n$ is the current parameter vector
- $\alpha > 0$ is the <span style={{color: '#059669'}}>learning rate (step size)</span>
- $\nabla f(x_t)$ is the gradient vector at $x_t$

### 1.3 Why Does It Work?

Consider the **first-order Taylor expansion** of $f$ around $x_t$:

$$
f(x_t + \Delta x) \approx f(x_t) + \nabla f(x_t)^T \Delta x + O(\|\Delta x\|^2)
$$

For small step sizes, if we choose $\Delta x = -\alpha \nabla f(x_t)$:

$$
f(x_t + \Delta x) \approx f(x_t) - \alpha \|\nabla f(x_t)\|^2 < f(x_t)
$$

This guarantees we move <span style={{color: '#7c3aed'}}>downhill</span> as long as $\nabla f(x_t) \neq 0$.

---

## 2. Convergence Analysis

### 2.1 Convex Functions

For a **convex** function with **$L$-Lipschitz continuous gradient** (meaning $\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$), gradient descent with step size $\alpha = \frac{1}{L}$ achieves:

$$
f(x_k) - f(x^*) \leq \frac{L\|x_0 - x^*\|^2}{2k}
$$

This is a <span style={{color: '#ea580c'}}>$O(1/k)$ convergence rate</span> ([Boyd & Vandenberghe, 2004](#ref-1)).

### 2.2 Strong Convexity

If $f$ is **$\mu$-strongly convex** (second derivative bounded below), we get <span style={{color: '#059669'}}>linear convergence</span>:

$$
f(x_k) - f(x^*) \leq \left(1 - \frac{\mu}{L}\right)^k (f(x_0) - f(x^*))
$$

The **condition number** $\kappa = L/\mu$ determines convergence speed.

---

## 3. Implementation: Vanilla Gradient Descent

Let's implement gradient descent from scratch:

```python
import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, Tuple, List

def gradient_descent(
    f: Callable[[np.ndarray], float],
    grad_f: Callable[[np.ndarray], np.ndarray],
    x0: np.ndarray,
    alpha: float = 0.01,
    max_iters: int = 1000,
    tol: float = 1e-6,
    verbose: bool = True
) -> Tuple[np.ndarray, List[float]]:
    """
    Vanilla gradient descent optimizer.
    
    Args:
        f: Objective function
        grad_f: Gradient of objective function
        x0: Initial point
        alpha: Learning rate (step size)
        max_iters: Maximum iterations
        tol: Convergence tolerance
        verbose: Print progress
        
    Returns:
        x_opt: Optimized parameters
        history: Loss history
    """
    x = x0.copy()
    history = []
    
    for iteration in range(max_iters):
        # Compute gradient
        grad = grad_f(x)
        
        # Check convergence
        grad_norm = np.linalg.norm(grad)
        if grad_norm < tol:
            if verbose:
                print(f"Converged at iteration {iteration}")
            break
        
        # Update parameters
        x = x - alpha * grad
        
        # Record loss
        loss = f(x)
        history.append(loss)
        
        if verbose and iteration % 100 == 0:
            print(f"Iter {iteration}: loss = {loss:.6f}, ||grad|| = {grad_norm:.6f}")
    
    return x, history

# Example: Minimize f(x) = x^T A x + b^T x (quadratic function)
np.random.seed(42)
n = 10
A = np.random.randn(n, n)
A = A.T @ A + np.eye(n)  # Make positive definite
b = np.random.randn(n)

def quadratic(x):
    return 0.5 * x @ A @ x + b @ x

def grad_quadratic(x):
    return A @ x + b

# Run optimization
x0 = np.random.randn(n)
x_opt, history = gradient_descent(
    quadratic, grad_quadratic, x0, 
    alpha=0.01, max_iters=500, verbose=False
)

print(f"Optimal x found: {x_opt[:3]}... (showing first 3 dims)")
print(f"Final loss: {quadratic(x_opt):.8f}")
```

**Output:**
```
Optimal x found: [-0.234, 0.891, -1.203]... (showing first 3 dims)
Final loss: -2.51947382
```

---

## 4. Stochastic Gradient Descent (SGD)

In machine learning, computing the full gradient over the entire dataset is expensive. <span style={{color: '#dc2626'}}>SGD uses mini-batches</span> to approximate the gradient.

### 4.1 The Algorithm

Instead of:
$$
\theta_{t+1} = \theta_t - \alpha \nabla_\theta \frac{1}{N}\sum_{i=1}^N \ell(\theta; x_i, y_i)
$$

We sample a mini-batch $\mathcal{B}_t$ and use:
$$
\theta_{t+1} = \theta_t - \alpha \nabla_\theta \frac{1}{|\mathcal{B}_t|}\sum_{i \in \mathcal{B}_t} \ell(\theta; x_i, y_i)
$$

### 4.2 Implementation

```python
def sgd(
    f_batch: Callable[[np.ndarray, List[int]], float],
    grad_f_batch: Callable[[np.ndarray, List[int]], np.ndarray],
    x0: np.ndarray,
    n_samples: int,
    batch_size: int = 32,
    alpha: float = 0.01,
    n_epochs: int = 10,
    shuffle: bool = True
) -> Tuple[np.ndarray, List[float]]:
    """
    Stochastic Gradient Descent with mini-batches.
    
    Args:
        f_batch: Loss function that takes (params, batch_indices)
        grad_f_batch: Gradient function that takes (params, batch_indices)
        x0: Initial parameters
        n_samples: Total number of training samples
        batch_size: Mini-batch size
        alpha: Learning rate
        n_epochs: Number of passes through data
        shuffle: Shuffle data each epoch
        
    Returns:
        x_opt: Optimized parameters
        history: Loss history (per epoch)
    """
    x = x0.copy()
    history = []
    
    for epoch in range(n_epochs):
        # Shuffle indices
        indices = np.arange(n_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        epoch_loss = 0.0
        n_batches = 0
        
        # Mini-batch loop
        for i in range(0, n_samples, batch_size):
            batch_idx = indices[i:i+batch_size]
            
            # Compute gradient on mini-batch
            grad = grad_f_batch(x, batch_idx)
            
            # Update parameters
            x = x - alpha * grad
            
            # Accumulate loss
            epoch_loss += f_batch(x, batch_idx)
            n_batches += 1
        
        # Record average epoch loss
        avg_loss = epoch_loss / n_batches
        history.append(avg_loss)
        print(f"Epoch {epoch+1}/{n_epochs}: loss = {avg_loss:.6f}")
    
    return x, history
```

---

## 5. Momentum-Based Methods

Vanilla SGD can oscillate in ravines where the surface curves more steeply in one dimension. <span style={{color: '#2563eb'}}>Momentum helps accelerate convergence.</span>

### 5.1 Classical Momentum

$$
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(x_t) \\
x_{t+1} &= x_t - \alpha v_{t+1}
\end{align}
$$

where $\beta \in [0, 1)$ is the momentum coefficient (typically 0.9).

### 5.2 Nesterov Accelerated Gradient (NAG)

NAG looks ahead before computing the gradient [Nesterov, 1983](#ref-2):

$$
\begin{align}
v_{t+1} &= \beta v_t + \nabla f(x_t - \alpha \beta v_t) \\
x_{t+1} &= x_t - \alpha v_{t+1}
\end{align}
$$

### 5.3 Implementation

```python
class MomentumOptimizer:
    """Gradient descent with momentum."""
    
    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):
        self.alpha = learning_rate
        self.beta = momentum
        self.velocity = None
    
    def step(self, params: np.ndarray, grad: np.ndarray) -> np.ndarray:
        """Perform one optimization step."""
        if self.velocity is None:
            self.velocity = np.zeros_like(params)
        
        # Update velocity
        self.velocity = self.beta * self.velocity + grad
        
        # Update parameters
        params = params - self.alpha * self.velocity
        
        return params

# Example usage
optimizer = MomentumOptimizer(learning_rate=0.01, momentum=0.9)
x = np.random.randn(10)

for i in range(100):
    grad = grad_quadratic(x)
    x = optimizer.step(x, grad)
    
    if i % 20 == 0:
        print(f"Iter {i}: loss = {quadratic(x):.6f}")
```

---

## 6. Adaptive Learning Rate Methods

Different parameters may need different learning rates. <span style={{color: '#7c3aed'}}>Adaptive methods automatically tune per-parameter learning rates.</span>

### 6.1 AdaGrad

AdaGrad ([Duchi et al., 2011](#ref-3)) accumulates squared gradients:

$$
\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii} + \epsilon}} g_{t,i}
$$

where $G_t = \sum_{\tau=1}^t g_\tau g_\tau^T$ and $g_t = \nabla f(\theta_t)$.

**Problem:** Learning rate decays too aggressively for deep learning.

### 6.2 RMSProp

RMSProp uses an exponentially decaying average [Hinton, 2012](#ref-4):

$$
\begin{align}
v_t &= \gamma v_{t-1} + (1-\gamma) g_t^2 \\
\theta_{t+1} &= \theta_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} g_t
\end{align}
$$

### 6.3 Adam (Adaptive Moment Estimation)

<span style={{color: '#ea580c', fontWeight: '600'}}>Adam is the most popular optimizer in deep learning</span> ([Kingma & Ba, 2015](#ref-5)). It combines momentum and RMSProp:

$$
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t} \quad \text{(bias correction)} \\
\hat{v}_t &= \frac{v_t}{1-\beta_2^t} \quad \text{(bias correction)} \\
\theta_{t+1} &= \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
$$

Default hyperparameters: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$.

### 6.4 Complete Adam Implementation

```python
class AdamOptimizer:
    """
    Adam: Adaptive Moment Estimation optimizer.
    
    Reference: Kingma & Ba (2015) "Adam: A Method for Stochastic Optimization"
    """
    
    def __init__(
        self, 
        learning_rate: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8
    ):
        self.alpha = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        
        # State variables
        self.m = None  # First moment (mean)
        self.v = None  # Second moment (variance)
        self.t = 0     # Time step
    
    def step(self, params: np.ndarray, grad: np.ndarray) -> np.ndarray:
        """Perform one Adam optimization step."""
        # Initialize moments on first call
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)
        
        self.t += 1
        
        # Update biased first moment estimate
        self.m = self.beta1 * self.m + (1 - self.beta1) * grad
        
        # Update biased second raw moment estimate
        self.v = self.beta2 * self.v + (1 - self.beta2) * (grad ** 2)
        
        # Compute bias-corrected moments
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        
        # Update parameters
        params = params - self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)
        
        return params
    
    def reset(self):
        """Reset optimizer state."""
        self.m = None
        self.v = None
        self.t = 0

# Comparison: Adam vs SGD vs Momentum
def compare_optimizers(f, grad_f, x0, n_iters=200):
    """Compare different optimizers on the same problem."""
    
    # Initialize optimizers
    sgd_params = x0.copy()
    momentum_params = x0.copy()
    adam_params = x0.copy()
    
    momentum_opt = MomentumOptimizer(learning_rate=0.01, momentum=0.9)
    adam_opt = AdamOptimizer(learning_rate=0.01)
    
    histories = {'SGD': [], 'Momentum': [], 'Adam': []}
    
    for i in range(n_iters):
        # SGD
        sgd_params = sgd_params - 0.01 * grad_f(sgd_params)
        histories['SGD'].append(f(sgd_params))
        
        # Momentum
        momentum_params = momentum_opt.step(momentum_params, grad_f(momentum_params))
        histories['Momentum'].append(f(momentum_params))
        
        # Adam
        adam_params = adam_opt.step(adam_params, grad_f(adam_params))
        histories['Adam'].append(f(adam_params))
    
    return histories

# Run comparison
x0 = np.random.randn(10)
histories = compare_optimizers(quadratic, grad_quadratic, x0)

# Plot results
plt.figure(figsize=(10, 6))
for name, history in histories.items():
    plt.semilogy(history, label=name, linewidth=2)
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('Loss (log scale)', fontsize=12)
plt.title('Optimizer Comparison', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('optimizer_comparison.png', dpi=150)
print("Plot saved as 'optimizer_comparison.png'")
```

---

## 7. Practical Considerations

### 7.1 Learning Rate Schedules

<span style={{color: '#059669'}}>Decaying the learning rate often improves convergence:</span>

1. **Step decay:** $\alpha_t = \alpha_0 \cdot \gamma^{\lfloor t/k \rfloor}$
2. **Exponential decay:** $\alpha_t = \alpha_0 e^{-\lambda t}$
3. **Cosine annealing:** $\alpha_t = \alpha_{\min} + \frac{1}{2}(\alpha_{\max} - \alpha_{\min})(1 + \cos(\frac{t}{T}\pi))$

```python
def cosine_annealing_lr(epoch: int, n_epochs: int, lr_max: float, lr_min: float = 0.0):
    """Cosine annealing learning rate schedule."""
    return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(epoch / n_epochs * np.pi))
```

### 7.2 Gradient Clipping

Prevent exploding gradients in RNNs:

```python
def clip_gradient(grad: np.ndarray, max_norm: float = 1.0) -> np.ndarray:
    """Clip gradient by norm."""
    grad_norm = np.linalg.norm(grad)
    if grad_norm > max_norm:
        grad = grad * (max_norm / grad_norm)
    return grad
```

### 7.3 Choosing an Optimizer

| Optimizer | Best For | Pros | Cons |
|-----------|----------|------|------|
| **SGD** | Simple problems | <span style={{color: '#059669'}}>Stable, well-understood</span> | Slow, sensitive to LR |
| **SGD+Momentum** | CNNs, ResNets | <span style={{color: '#059669'}}>Faster, escapes saddles</span> | Needs tuning |
| **Adam** | Transformers, most DL | <span style={{color: '#059669'}}>Adaptive, robust</span> | <span style={{color: '#dc2626'}}>May not generalize as well</span> |
| **AdamW** | Large models (GPT, BERT) | <span style={{color: '#059669'}}>Better regularization</span> | More hyperparameters |

---

## 8. Advanced Topics

### 8.1 Second-Order Methods

Newton's method uses the Hessian $H = \nabla^2 f(x)$:

$$
x_{t+1} = x_t - \alpha H^{-1} \nabla f(x_t)
$$

<span style={{color: '#7c3aed'}}>Quadratic convergence but $O(n^3)$ cost</span> for Hessian inversion.

### 8.2 Quasi-Newton Methods (L-BFGS)

Approximate $H^{-1}$ using gradient history [Liu & Nocedal, 1989](#ref-6):

```python
from scipy.optimize import minimize

result = minimize(
    quadratic, 
    x0, 
    method='L-BFGS-B',
    jac=grad_quadratic,
    options={'maxiter': 100}
)
print(f"L-BFGS solution: {result.x[:3]}...")
print(f"Function value: {result.fun:.8f}")
```

---

## Conclusion

We've journeyed from the mathematical foundations of gradient descent to state-of-the-art optimizers like Adam. Key takeaways:

1. <span style={{color: '#2563eb', fontWeight: '600'}}>Gradient descent is guaranteed to converge</span> for convex functions with appropriate learning rates
2. <span style={{color: '#059669', fontWeight: '600'}}>Momentum accelerates convergence</span> by smoothing noisy gradients
3. <span style={{color: '#ea580c', fontWeight: '600'}}>Adaptive methods (Adam, RMSProp)</span> automatically tune per-parameter learning rates
4. <span style={{color: '#7c3aed', fontWeight: '600'}}>Practical considerations</span> (LR schedules, gradient clipping) are crucial for deep learning

For most deep learning tasks, **start with Adam** and fall back to SGD+Momentum if generalization is critical.

---

## References

<div id="ref-1">
1. Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
</div>

<div id="ref-2">
2. Nesterov, Y. (1983). A method for solving the convex programming problem with convergence rate O(1/k¬≤). *Soviet Mathematics Doklady*, 27, 372-376.
</div>

<div id="ref-3">
3. Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. *Journal of Machine Learning Research*, 12, 2121-2159.
</div>

<div id="ref-4">
4. Hinton, G., Srivastava, N., & Swersky, K. (2012). Neural networks for machine learning lecture 6a: Overview of mini-batch gradient descent. *Coursera*.
</div>

<div id="ref-5">
5. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *ICLR 2015*. [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)
</div>

<div id="ref-6">
6. Liu, D. C., & Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. *Mathematical Programming*, 45(1-3), 503-528.
</div>

---

*Questions or comments? Reach out at [skatel@princeton.edu](mailto:skatel@princeton.edu)*

  </div>
</div>
